# LLM TTFB Benchmark Report: Transcript Cleanup Task

**Test Date:** July 22, 2025  
**Task:** Real-time transcript cleanup using LLM post-processing  
**Transcript Length:** 1,825 characters  
**Test Methodology:** 25 consecutive trials + 5 cold start trials per provider

---

## Executive Summary

This benchmark compares Time To First Byte (TTFB) performance across different LLM providers for a real-world transcript cleanup task. The results reveal significant differences in both warm performance and cold start penalties, crucial for voice dictation app optimization.

### Key Findings:
- **Groq Llama 3.1-8B** delivers the fastest warm performance at 267ms average
- **Cold start penalties** range from 122% to 285% slower than warm performance
- **Prewarming strategy** can eliminate perceived latency in voice apps

---

## Detailed Results

### ğŸ¥‡ Groq - Llama 3.1-8B Instant

| Metric | Warm Performance | Cold Start | Startup Penalty |
|--------|-----------------|------------|------------------|
| **Average TTFB** | 267.1 ms | 1,027.8 ms | +760.7 ms (284.8% slower) |
| **Median TTFB** | 208.0 ms | - | - |
| **Min TTFB** | 193.6 ms | 821.2 ms | - |
| **Max TTFB** | 657.4 ms | 1,154.0 ms | - |
| **Std Deviation** | 145.5 ms | - | - |

**Performance Profile:**
- âœ… **Fastest warm performance** (267ms average)
- âœ… **Most consistent** (lowest std deviation)
- âœ… **Quick warmup** (reaches optimal speed by trial 4)
- âš ï¸ **Highest cold start penalty** (285% slower)

### ğŸ¥ˆ Groq - Llama 4 Scout 17B

| Metric | Warm Performance | Cold Start | Startup Penalty |
|--------|-----------------|------------|------------------|
| **Average TTFB** | 535.2 ms | 1,189.1 ms | +653.9 ms (122.2% slower) |
| **Median TTFB** | 418.3 ms | - | - |
| **Min TTFB** | 378.1 ms | 818.7 ms | - |
| **Max TTFB** | 1,709.3 ms | 1,621.2 ms | - |
| **Std Deviation** | 294.9 ms | - | - |

**Performance Profile:**
- ğŸ”¶ **Moderate warm performance** (535ms average)
- âš ï¸ **High variability** (294ms std deviation, max 1709ms)
- âœ… **Lower cold start penalty** (122% vs 285% for 3.1)
- ğŸ”¶ **Larger model** provides better quality at cost of speed

### ğŸ¥‰ Baseten - Llama 4 Scout 17B

| Metric | Warm Performance | Cold Start | Startup Penalty |
|--------|-----------------|------------|------------------|
| **Average TTFB** | 385.7 ms | 1,029.6 ms | +643.9 ms (166.9% slower) |
| **Median TTFB** | 373.4 ms | - | - |
| **Min TTFB** | 356.9 ms | 890.9 ms | - |
| **Max TTFB** | 528.3 ms | 1,179.8 ms | - |
| **Std Deviation** | 39.1 ms | - | - |

**Performance Profile:**
- ğŸ”¶ **Moderate warm performance** (386ms average)
- âœ… **Very consistent** (39ms std deviation - most stable)
- ğŸ”¶ **Moderate cold start penalty** (167% slower)
- ğŸ”¶ **Dedicated infrastructure** provides predictable performance

---

## Performance Analysis

### Warm Performance Ranking
1. **Groq Llama 3.1-8B**: 267ms (Champion) ğŸ†
2. **Baseten Llama 4 Scout**: 386ms 
3. **Groq Llama 4 Scout**: 535ms

### Cold Start Impact
1. **Groq 4 Scout**: 122% slower (lowest penalty)
2. **Baseten Llama 4**: 167% slower (moderate penalty)
3. **Groq 3.1**: 285% slower (highest penalty, but fastest warm)

### Variability Analysis
1. **Baseten**: Most stable (39ms std dev) âœ…
2. **Groq 3.1**: Consistent (145ms std dev)
3. **Groq 4**: Variable (295ms std dev, occasional spikes to 1.7s)

---

## Recommendations for Voice Dictation App

### ğŸš€ Optimal Strategy: Parallel Prewarming

Based on these results, implement the following architecture:

```
User presses hotkey
â”œâ”€â”€ Start ASR streaming (Soniox) â”€â”€â”€ 2-5 seconds â”€â”€â”
â””â”€â”€ Start LLM prewarming (Groq) â”€â”€â”€ 0.2-0.5s â”€â”€â”€â”€â”€â”€â”¤
                                                    â”œâ”€â”€ Instant enhancement
                                                    â””â”€â”€ No perceived latency
```

### ğŸ¯ Provider Recommendations

**For Production:**
- **Primary**: Groq Llama 3.1-8B (fastest warm performance at 267ms)
- **Reliability**: Baseten Llama 4 (most consistent, moderate speed at 386ms)
- **Quality**: Groq Llama 4 Scout (better model, slower at 535ms)

**Use Case Recommendations:**
- **Speed-critical apps**: Groq 3.1-8B
- **Enterprise/reliability-focused**: Baseten (most predictable)
- **Quality over speed**: Groq 4 Scout

### ğŸ’° Cost Optimization for Prewarming

**Minimal Ping Strategy:**
```python
# Ultra-lightweight prewarm request
prewarm_request = {
    "messages": [{"role": "user", "content": "Hi"}],
    "max_tokens": 1,
    "temperature": 0
}
```

**Token Cost Impact:**
- Input: 1 token (~$0.000001)
- Output: 1 token (~$0.000001) 
- **Total cost per prewarm: ~$0.000002 (negligible)**

### ğŸ—ï¸ Implementation Architecture

1. **Recording Start**: Trigger both ASR + LLM prewarm
2. **Recording End**: ASR completes, LLM already warm
3. **Enhancement**: Instant processing with pre-warmed connection
4. **Fallback**: If prewarm fails, graceful degradation to cold start

---

## Technical Implementation

### Prewarming Code Pattern
```swift
func startRecording() async {
    async let transcription = sonioxStreamingService.startStreaming()
    async let llmPrewarm = aiEnhancementService.prewarmConnection()
    
    // Both run in parallel during recording
    let (transcript, _) = await (transcription, llmPrewarm)
    
    // LLM is now warm, instant enhancement
    let enhanced = await aiEnhancementService.enhance(transcript)
}
```

### Error Handling
- **Prewarm timeout**: Fall back to cold start
- **Network issues**: Retry with exponential backoff
- **Provider downtime**: Switch to backup provider

---

## Conclusions

1. **Groq Llama 3.1-8B** offers the best balance of speed and consistency for production use
2. **Prewarming strategy** can eliminate the 200-1000ms cold start penalty 
3. **Cost impact** of prewarming is negligible (~$0.000002 per request)
4. **Parallel initialization** during ASR processing provides optimal UX

The data strongly supports implementing prewarming for production voice dictation apps, with Groq as the recommended provider for real-time transcript enhancement.

---

*Report generated from benchmark data collected on July 22, 2025*
*Test scripts: `test_groq.py`, `test_basten.py`*