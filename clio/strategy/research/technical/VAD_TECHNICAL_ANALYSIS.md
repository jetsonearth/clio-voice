# Voice Activity Detection (VAD) Implementation in Clio - Technical Analysis

## Executive Summary

This document provides a comprehensive technical analysis of the Voice Activity Detection (VAD) implementation in Clio, a sophisticated macOS voice transcription application. The analysis covers the current VAD architecture, audio processing pipeline, and potential areas for streaming and real-time transcription enhancements.

## Table of Contents

1. [Current VAD Architecture](#current-vad-architecture)
2. [Audio Processing Pipeline](#audio-processing-pipeline)
3. [VAD Integration Points](#vad-integration-points)
4. [Streaming Capabilities](#streaming-capabilities)
5. [Technical Recommendations](#technical-recommendations)

## Current VAD Architecture

### 1. VAD Model and Core Components

Clio uses the **Silero VAD v5.1.2** model (`ggml-silero-v5.1.2.bin`) integrated through whisper.cpp's VAD functionality. The implementation consists of several key components:

#### WhisperVAD.swift
- **Purpose**: Swift wrapper around whisper.cpp's VAD functionality
- **Key Features**:
  - Thread-safe VAD context management using Swift actors
  - Configurable VAD parameters (threshold, duration, padding)
  - Speech segment detection and extraction
  - GPU acceleration support

```swift
struct VADParams {
    var threshold: Float = 0.5                 // Probability threshold for speech
    var minSpeechDurationMs: Int32 = 250     // Min duration for valid speech
    var minSilenceDurationMs: Int32 = 100    // Min silence to end speech
    var maxSpeechDurationS: Float = 30.0     // Max speech segment duration
    var speechPadMs: Int32 = 30              // Padding around speech
    var samplesOverlap: Float = 0.1          // Overlap when copying samples
}
```

#### VADSegment Structure
```swift
struct VADSegment {
    let startTime: Float  // Start time in seconds
    let endTime: Float    // End time in seconds
    let startSample: Int  // Start sample index
    let endSample: Int    // End sample index
}
```

### 2. VAD Model Management

The VAD model is automatically downloaded and managed by WhisperState:

- **Download URL**: `https://huggingface.co/ggml-org/whisper-vad/resolve/main/ggml-silero-v5.1.2.bin`
- **Storage Location**: `~/Library/Application Support/com.jetsonai.clio/WhisperModels/ggml-silero-v5.1.2.bin`
- **Auto-download**: VAD model is downloaded automatically on first launch if not present
- **Initialization**: VAD context is initialized during WhisperState initialization

## Audio Processing Pipeline

### 1. Recording Phase
1. **Audio Capture**: Uses AVAudioRecorder to capture audio at 16kHz mono
2. **File Format**: WAV format with PCM encoding
3. **Temporary Storage**: Audio saved to temporary file during recording

### 2. Processing Phase

The audio processing flow in `WhisperState.transcribeAudio()`:

```swift
1. Load audio samples from WAV file
2. Apply VAD if available:
   - Detect speech segments
   - Extract speech-only samples
   - Log silence removal statistics
3. Pass processed samples to Whisper for transcription
4. Apply word replacements if enabled
5. Apply AI enhancement if enabled
6. Paste result to cursor position
```

### 3. VAD Processing Details

When VAD is enabled, the following occurs:

```swift
// From WhisperState.swift
if let vadContext = vadContext {
    do {
        logger.info("ðŸŽ™ï¸ VAD Context exists, applying VAD to remove silence...")
        logger.info("ðŸ“Š Original audio samples count: \(data.count)")
        let segments = try await vadContext.detectSpeechSegments(samples: data)
        logger.info("ðŸ” VAD detected \(segments.count) speech segments")
        processedSamples = await vadContext.extractSpeechSamples(from: data, segments: segments)
        logger.info("ðŸ“Š Processed samples count: \(processedSamples.count)")
        
        let stats = await vadContext.getVADStats(originalSamples: data.count, speechSamples: processedSamples.count)
        logger.info("âœ… VAD removed \(String(format: "%.1f", stats.silencePercentage))% silence")
    } catch {
        logger.error("âŒ VAD processing failed, using original audio: \(error)")
        processedSamples = data
    }
}
```

### 4. Sample Processing

Audio samples are processed as follows:

1. **WAV Decoding**: 16-bit PCM to float conversion (-1.0 to 1.0 range)
2. **VAD Segmentation**: Segments identified using Silero VAD
3. **Sample Extraction**: Speech segments concatenated into continuous audio
4. **Whisper Processing**: Processed samples sent to Whisper model

## VAD Integration Points

### 1. WhisperState Integration

- VAD context is stored as a property: `private var vadContext: WhisperVADContext?`
- Initialized during WhisperState init
- Applied during transcription before Whisper processing

### 2. TranscriptionManager Integration

- VAD context can be set via `setVADContext()`
- WhisperCppAdapter has a `transcribeWithVAD()` method
- VAD is applied when using whisper.cpp engine

### 3. Model Management Integration

- VAD model download handled in `WhisperState+ModelManager.swift`
- `downloadVADModel()` and `deleteVADModel()` methods available
- VAD model presence checked via `isVADModelDownloaded` property

## Streaming Capabilities

### 1. Current State

- **No Real-time Streaming**: Current implementation is batch-based
- **Post-recording Processing**: VAD and transcription happen after recording stops
- **Paraformer WebSocket Service**: Exists but not integrated into main pipeline

### 2. Streaming Infrastructure

#### ParaformerWebSocketASRService
- WebSocket-based real-time ASR service
- Supports multiple languages
- Not currently used in main transcription flow

#### StreamingBadge Component
- UI component for indicating streaming capability
- Currently decorative, not functional

### 3. Potential for Streaming

The codebase has foundations for streaming:
- WebSocket service implementation
- Audio engine setup for real-time capture
- VAD could be adapted for real-time processing

## Technical Recommendations

### 1. Real-time VAD Processing

To enable real-time VAD during recording:

```swift
// Conceptual implementation
class RealTimeVADProcessor {
    private let vadContext: WhisperVADContext
    private let bufferSize = 16000 // 1 second at 16kHz
    private var audioBuffer: [Float] = []
    
    func processAudioChunk(_ samples: [Float]) async -> VADResult {
        audioBuffer.append(contentsOf: samples)
        
        if audioBuffer.count >= bufferSize {
            let segments = try await vadContext.detectSpeechSegments(samples: audioBuffer)
            // Process segments and trigger transcription if speech detected
            audioBuffer.removeFirst(bufferSize / 2) // Sliding window
        }
    }
}
```

### 2. Streaming Transcription Architecture

Implement a streaming pipeline:

1. **Audio Capture** â†’ **Ring Buffer** â†’ **VAD Processing** â†’ **Speech Accumulator** â†’ **Whisper Transcription**
2. Use VAD to detect speech boundaries in real-time
3. Accumulate speech segments until silence detected
4. Trigger transcription on speech segments
5. Display partial results as they become available

### 3. VAD Parameter Optimization

Current VAD parameters could be tuned for better performance:

```swift
// Suggested adjustments for real-time processing
struct OptimizedVADParams {
    var threshold: Float = 0.6                 // Higher threshold for cleaner detection
    var minSpeechDurationMs: Int32 = 150     // Shorter for responsiveness
    var minSilenceDurationMs: Int32 = 300    // Longer to avoid cutting off speech
    var maxSpeechDurationS: Float = 10.0     // Shorter segments for streaming
    var speechPadMs: Int32 = 50              // More padding for safety
}
```

### 4. Integration with Existing Services

To enable streaming transcription:

1. **Modify WhisperState**: Add streaming mode toggle
2. **Create StreamingTranscriptionManager**: Handle real-time audio processing
3. **Integrate VAD**: Use VAD for real-time speech detection
4. **Update UI**: Show partial transcription results
5. **Buffer Management**: Implement ring buffer for continuous audio

### 5. Performance Considerations

- VAD processing is lightweight (~1-2ms per second of audio)
- Can run in real-time without significant overhead
- GPU acceleration available for better performance
- Consider processing audio in chunks (e.g., 100ms windows)

## Conclusion

Clio's VAD implementation is well-structured and functional for batch processing. The architecture supports silence removal effectively, improving transcription quality and reducing processing time. While the current implementation doesn't support real-time streaming, the codebase has the necessary components to implement streaming transcription with minimal architectural changes.

The key to enabling streaming would be:
1. Implementing real-time VAD processing during recording
2. Creating a buffer management system for continuous audio
3. Triggering incremental transcription on detected speech segments
4. Updating the UI to display partial results

The existing VAD infrastructure provides a solid foundation for these enhancements.